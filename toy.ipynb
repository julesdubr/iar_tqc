{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sb3_contrib import TQC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from single_state_mdp import SingleStateMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    # Define and return an instance of the SingleStateMDP environment\n",
    "    return SingleStateMDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TQC agent with specified parameters\n",
    "policy_kwargs = {\"n_quantiles\": 25, \"n_critics\": 2, \"net_arch\": [50,50]}\n",
    "agent = TQC(\"MlpPolicy\", env, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.tqc.tqc.TQC at 0x7f4491316b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for the desired number of iterations\n",
    "agent.learn(total_timesteps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.569 +/- 0.240\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the TQC agent using the evaluate_policy function\n",
    "mean_rewards, std_rewards = evaluate_policy(agent, make_env(), n_eval_episodes=100)\n",
    "\n",
    "print(f'Mean reward: {mean_rewards:.3f} +/- {std_rewards:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.092 +/- 0.470\n"
     ]
    }
   ],
   "source": [
    "# Define a dense uniform grid of actions to evaluate the approximations on\n",
    "actions = np.linspace(-1, 1, 2000)\n",
    "\n",
    "# Initialize arrays to store the results\n",
    "signed_discrepancies = np.zeros(actions.shape)\n",
    "\n",
    "# Evaluate the performance of the TQC agent on the dense uniform grid of actions\n",
    "for i, action in enumerate(actions):\n",
    "    # Get the approximate Q-value for the current action\n",
    "    q_value, _ = agent.predict(np.array([0]), np.array([action]))\n",
    "    \n",
    "    # Calculate the signed discrepancy between the approximate Q-value and the true Q-value\n",
    "    signed_discrepancy = q_value - env._mean_reward(action)\n",
    "    \n",
    "    # Store the results\n",
    "    signed_discrepancies[i] = signed_discrepancy\n",
    "\n",
    "print(f'Mean reward: {signed_discrepancies.mean():.3f} +/- {signed_discrepancies.std():.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tqc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a14f325d447890c1aa81d5004c3a59981b75ede9581a60409be6700f02a9947e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
